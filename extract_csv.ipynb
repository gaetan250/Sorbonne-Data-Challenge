{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce script lit des fichiers `.json` contenant des graphes DOT, extrait des features avancées (structurelles et instruction-level), puis sauvegarde les résultats par batch au format CSV.  \n",
    "Il supprime automatiquement les JSON déjà traités pour éviter les doublons et optimise le tout via `joblib.Parallel`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import gc\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def parse_cfg_dot(dot_str):\n",
    "    \"\"\"\n",
    "    Parse un fichier DOT en chaîne de caractères et retourne un graphe orienté NetworkX.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    lines = dot_str.strip().split('\\n')\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if '->' in line:\n",
    "            match = re.match(r'\"([^\"]+)\"\\s*->\\s*\"([^\"]+)\"', line)\n",
    "            if match:\n",
    "                src, dst = match.groups()\n",
    "                G.add_edge(src, dst)\n",
    "        elif '[' in line and 'label =' in line:\n",
    "            match = re.match(r'\"([^\"]+)\"\\s*\\[label = \"(.*?)\"\\]', line)\n",
    "            if match:\n",
    "                node_id, label = match.groups()\n",
    "                parts = label.split(' : ')\n",
    "                addr = parts[0]\n",
    "                instr_type = parts[1] if len(parts) > 1 else \"UNK\"\n",
    "                instr_text = parts[2] if len(parts) > 2 else \"\"\n",
    "                G.add_node(node_id, addr=addr, type=instr_type, text=instr_text)\n",
    "    return G\n",
    "\n",
    "\n",
    "def load_single_graph(file_path):\n",
    "    \"\"\"\n",
    "    Charge un fichier .json contenant un graphe DOT et retourne un tuple (graph_id, graphe).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            dot_str = f.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "        if not dot_str.strip():\n",
    "            return None\n",
    "        G = parse_cfg_dot(dot_str)\n",
    "        if G.number_of_nodes() == 0:\n",
    "            return None\n",
    "        return os.path.basename(file_path).replace(\".json\", \"\"), G\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur avec {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_features_from_graph(graph_id, G):\n",
    "    \"\"\"\n",
    "    Extrait des features avancées depuis un graphe NetworkX.\n",
    "    \"\"\"\n",
    "    instr_types = nx.get_node_attributes(G, \"type\").values()\n",
    "    instr_texts = nx.get_node_attributes(G, \"text\").values()\n",
    "\n",
    "    instr_types_series = pd.Series(instr_types)\n",
    "    instr_texts_series = pd.Series(instr_texts)\n",
    "    instr_counts = instr_types_series.value_counts()\n",
    "    total_instrs = len(instr_texts_series)\n",
    "\n",
    "    def count_ratio_text(substring):\n",
    "        matches = instr_texts_series.str.contains(substring, case=False, na=False)\n",
    "        count = matches.sum()\n",
    "        return count, count / total_instrs if total_instrs > 0 else 0\n",
    "\n",
    "    def count_ratio_type(op):\n",
    "        count = instr_counts.get(op.upper(), 0)\n",
    "        return count, count / total_instrs if total_instrs > 0 else 0\n",
    "\n",
    "    exit_nodes = [n for n in G.nodes if G.out_degree(n) == 0]\n",
    "    try:\n",
    "        longest_path_len = nx.dag_longest_path_length(G)\n",
    "    except Exception:\n",
    "        longest_path_len = -1\n",
    "\n",
    "    features = {\n",
    "        \"graph_id\": graph_id,\n",
    "        \"nb_nodes\": G.number_of_nodes(),\n",
    "        \"nb_edges\": G.number_of_edges(),\n",
    "        \"nb_components\": nx.number_weakly_connected_components(G),\n",
    "        \"nb_exit_nodes\": len(exit_nodes),\n",
    "        \"max_path_len\": longest_path_len,\n",
    "        \"avg_degree\": G.number_of_edges() / G.number_of_nodes() if G.number_of_nodes() > 0 else 0,\n",
    "        \"count_inst\": total_instrs,\n",
    "        \"prop_inst\": total_instrs / G.number_of_nodes() if G.number_of_nodes() > 0 else 0,\n",
    "    }\n",
    "\n",
    "    for opcode in [\"CALL\", \"JMP\", \"RET\", \"JCC\", \"INVALID\", \"HLT\", \"INST\"]:\n",
    "        count, ratio = count_ratio_type(opcode)\n",
    "        features[f\"count_type_{opcode.lower()}\"] = count\n",
    "        features[f\"prop_type_{opcode.lower()}\"] = ratio\n",
    "\n",
    "    for keyword in [\"mov\", \"add\", \"xor\", \"cmp\", \"push\", \"pop\", \"lea\", \"sub\", \"loop\", \"syscall\", \"call\", \"jmp\"]:\n",
    "        count, ratio = count_ratio_text(keyword)\n",
    "        features[f\"count_text_{keyword}\"] = count\n",
    "        features[f\"prop_text_{keyword}\"] = ratio\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def clean_jsons_from_existing_csv(csv_dir, folder_path):\n",
    "    \"\"\"\n",
    "    Supprime les fichiers .json déjà traités en vérifiant les CSV de features existants.\n",
    "    \"\"\"\n",
    "    print(\"Nettoyage des JSON déjà traités à partir des CSV existants...\")\n",
    "    all_csv = sorted(glob.glob(os.path.join(csv_dir, \"features_batch_*.csv\")))\n",
    "    treated_ids = set()\n",
    "\n",
    "    for csv_file in all_csv:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file, usecols=[\"graph_id\"])\n",
    "            treated_ids.update(df[\"graph_id\"].dropna().astype(str).tolist())\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lecture {csv_file} : {e}\")\n",
    "\n",
    "    print(f\"{len(treated_ids)} fichiers déjà extraits détectés dans les CSV.\")\n",
    "\n",
    "    deleted = 0\n",
    "    for gid in treated_ids:\n",
    "        json_path = os.path.join(folder_path, f\"{gid}.json\")\n",
    "        if os.path.exists(json_path):\n",
    "            try:\n",
    "                os.remove(json_path)\n",
    "                deleted += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Impossible de supprimer {json_path} : {e}\")\n",
    "\n",
    "    print(f\"{deleted} fichiers JSON supprimés car déjà présents dans les CSV.\")\n",
    "\n",
    "\n",
    "def extract_features_to_csv_by_batch(folder_path, csv_dir=\"features_batches\", n_jobs=4, batch_size=500):\n",
    "    \"\"\"\n",
    "    Pipeline principale : lit les fichiers JSON → extrait les graphes → calcule les features → sauvegarde CSV.\n",
    "    \"\"\"\n",
    "    os.makedirs(csv_dir, exist_ok=True)\n",
    "    clean_jsons_from_existing_csv(csv_dir, folder_path)\n",
    "\n",
    "    files = sorted([os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".json\")])\n",
    "    total_files = len(files)\n",
    "    print(f\"{total_files} fichiers JSON trouvés dans {folder_path}\")\n",
    "\n",
    "    for i in range(0, total_files, batch_size):\n",
    "        batch_files = files[i:i + batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "        csv_path = os.path.join(csv_dir, f\"features_batch_{batch_num:02}.csv\")\n",
    "\n",
    "        if os.path.exists(csv_path):\n",
    "            print(f\"Batch {batch_num:02} déjà existant. Skip.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nBatch {batch_num:02} → {len(batch_files)} fichiers à parser...\")\n",
    "\n",
    "        graph_results = Parallel(n_jobs=n_jobs, backend=\"loky\")(\n",
    "            delayed(load_single_graph)(file) for file in tqdm(batch_files, desc=f\"Loading batch {batch_num:02}\")\n",
    "        )\n",
    "        graph_results = [res for res in graph_results if res is not None]\n",
    "\n",
    "        features = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(extract_features_from_graph)(gid, G) for gid, G in graph_results\n",
    "        )\n",
    "\n",
    "        df = pd.DataFrame(features)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Features batch {batch_num:02} enregistrées dans {csv_path}\")\n",
    "\n",
    "        processed_files = [os.path.join(folder_path, f\"{graph_id}.json\") for graph_id, _ in graph_results]\n",
    "        for f in processed_files:\n",
    "            try:\n",
    "                os.remove(f)\n",
    "            except Exception as e:\n",
    "                print(f\"Impossible de supprimer {f} : {e}\")\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"\\nTous les batchs disponibles ont été extraits vers CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"folder_test_set\"\n",
    "extract_features_to_csv_by_batch(folder_path, csv_dir=\"features_batches\", n_jobs=4, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Ce script merge les différents batchs !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "csv_dir = \"data/features_batches\" \n",
    "\n",
    "csv_files = sorted(glob.glob(f\"{csv_dir}/features_batch_*.csv\"))\n",
    "print(f\"{len(csv_files)} fichiers trouvés.\")\n",
    "\n",
    "df_all = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
    "\n",
    "df_all = df_all.drop_duplicates(subset=\"graph_id\")\n",
    "\n",
    "print(f\"Nombre total de graph_id uniques : {df_all['graph_id'].nunique()}\")\n",
    "\n",
    "output_path = \"data/train.csv\"\n",
    "df_all.to_csv(output_path, index=False)\n",
    "\n",
    "print(df_all.tail())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
