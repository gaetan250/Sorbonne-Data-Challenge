{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce script prépare un dataset PyTorch Geometric à partir de fichiers `.json` représentant des graphes CFG, en extrayant automatiquement des features avancées par nœud (structurelles, instructions, appels API) et en les associant aux labels d'un fichier.  \n",
    "Il génère aussi un modèle de features sauvegardé (`feature_model.pkl`) et les graphes encodés au format `.pt` dans un dossier `processed`.\n",
    "\n",
    "Remarque: Le fichier train contenant les labels est un csv et le fichier test un excel il faut lancer le read_csv (et ajouter le séparateur ;) en read_excel pour passer de l'un à l'autre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.utils import from_networkx\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DIGRAPH_DIR = 'folder_test_set'  \n",
    "LABELS_PATH = 'data/test_set_metadata_to_predict.xlsx'  \n",
    "OUTPUT_DIR = 'pt_output/test' \n",
    "SAMPLE_SIZE = 5000  \n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'processed'), exist_ok=True)\n",
    "\n",
    "def load_digraph(file_path):\n",
    "    \"\"\"Charge un fichier digraph et le convertit en graphe NetworkX\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        lines = content.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('Digraph') or line.startswith('{') or line.startswith('}'):\n",
    "                continue\n",
    "            \n",
    "            if '->' in line: \n",
    "                parts = line.split('->')\n",
    "                source = parts[0].strip().strip('\"')\n",
    "                target = parts[1].strip().strip('\"')\n",
    "                G.add_edge(source, target)\n",
    "            elif '[label' in line:  \n",
    "                node_match = re.match(r'\"([^\"]+)\"\\s*\\[label\\s*=\\s*\"([^\"]+)\"\\]', line)\n",
    "                if node_match:\n",
    "                    node_id = node_match.group(1)\n",
    "                    label = node_match.group(2)\n",
    "                    G.add_node(node_id, label=label)\n",
    "        \n",
    "        return G\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement de {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def analyze_instruction_stats(digraph_dir, max_files=SAMPLE_SIZE):\n",
    "    \"\"\"Analyse les statistiques des instructions dans les fichiers digraph\"\"\"\n",
    "    all_files = [os.path.join(digraph_dir, f) for f in os.listdir(digraph_dir) if f.endswith('.json')]\n",
    "    \n",
    "    if len(all_files) > max_files:\n",
    "        sample_files = random.sample(all_files, max_files)\n",
    "    else:\n",
    "        sample_files = all_files\n",
    "    \n",
    "    instruction_types = defaultdict(int)\n",
    "    registers_used = defaultdict(int)\n",
    "    api_calls = defaultdict(int)\n",
    "    memory_patterns = defaultdict(int)\n",
    "    control_flow_patterns = defaultdict(int)\n",
    "    suspicious_insts = defaultdict(int)\n",
    "    \n",
    "    total_nodes = 0\n",
    "    total_valid_nodes = 0\n",
    "    \n",
    "    for file_path in tqdm(sample_files, desc=\"Analyse des instructions\"):\n",
    "        G = load_digraph(file_path)\n",
    "        if not G:\n",
    "            continue\n",
    "            \n",
    "        total_nodes += len(G.nodes)\n",
    "        \n",
    "        for node_id, attrs in G.nodes(data=True):\n",
    "            if 'label' not in attrs:\n",
    "                continue\n",
    "            \n",
    "            label = attrs['label']\n",
    "            total_valid_nodes += 1\n",
    "            \n",
    "            if \"INST\" in label:\n",
    "                instruction = label.split(\"INST : \")[1].strip().lower()\n",
    "                \n",
    "                if \" \" in instruction:\n",
    "                    inst_type = instruction.split(\" \")[0].lower()\n",
    "                    instruction_types[inst_type] += 1\n",
    "                \n",
    "                if '[' in instruction and ']' in instruction:\n",
    "                    memory_patterns['memory_access'] += 1\n",
    "                    \n",
    "                    if 'mov' in instruction and '[' in instruction:\n",
    "                        if instruction.split('[')[0].strip().endswith('mov'):\n",
    "                            memory_patterns['memory_read'] += 1\n",
    "                        else:\n",
    "                            memory_patterns['memory_write'] += 1\n",
    "                \n",
    "                common_registers = ['eax', 'ebx', 'ecx', 'edx', 'esi', 'edi', 'ebp', 'esp', \n",
    "                                  'rax', 'rbx', 'rcx', 'rdx', 'rsi', 'rdi', 'rbp', 'rsp',\n",
    "                                  'ah', 'al', 'bh', 'bl', 'ch', 'cl', 'dh', 'dl']\n",
    "                for reg in common_registers:\n",
    "                    if re.search(r'\\b' + reg + r'\\b', instruction):\n",
    "                        registers_used[reg] += 1\n",
    "                \n",
    "                if 'xor' in instruction and 'eax, eax' in instruction:\n",
    "                    suspicious_insts['xor_eax_eax'] += 1  \n",
    "                if 'push' in instruction and ('offset' in instruction or 'str' in instruction):\n",
    "                    suspicious_insts['push_string'] += 1 \n",
    "                if 'call' in instruction and '[' in instruction:\n",
    "                    suspicious_insts['indirect_call'] += 1  \n",
    "                \n",
    "                if instruction.startswith('call'):\n",
    "                    api_name = instruction[4:].strip()\n",
    "                    if not api_name.startswith('0x'):  \n",
    "                        api_calls[api_name] += 1\n",
    "            \n",
    "            elif \"RET\" in label:\n",
    "                control_flow_patterns['ret'] += 1\n",
    "            elif \"JMP\" in label:\n",
    "                control_flow_patterns['jmp'] += 1\n",
    "            elif \"JCC\" in label:\n",
    "                control_flow_patterns['conditional_jmp'] += 1\n",
    "            elif \"CALL\" in label:\n",
    "                control_flow_patterns['call'] += 1\n",
    "    \n",
    "    return {\n",
    "        'instruction_types': dict(instruction_types),\n",
    "        'registers_used': dict(registers_used),\n",
    "        'api_calls': dict(api_calls),\n",
    "        'memory_patterns': dict(memory_patterns),\n",
    "        'control_flow_patterns': dict(control_flow_patterns),\n",
    "        'suspicious_insts': dict(suspicious_insts),\n",
    "        'total_nodes': total_nodes,\n",
    "        'total_valid_nodes': total_valid_nodes\n",
    "    }\n",
    "\n",
    "def create_advanced_feature_model(stats, inst_threshold=0.5, reg_threshold=0.5, api_min_count=10):\n",
    "    \"\"\"Crée un modèle de features avancé basé sur les statistiques\"\"\"\n",
    "    feature_model = {\n",
    "        'instruction_types': [],\n",
    "        'registers': [],\n",
    "        'apis': [],\n",
    "        'memory_patterns': ['memory_access', 'memory_read', 'memory_write'],\n",
    "        'control_flow': ['is_ret', 'is_jmp', 'is_conditional_jmp', 'is_call'],\n",
    "        'suspicious_patterns': ['xor_self', 'push_string', 'indirect_call', 'stack_manipulation'],\n",
    "        'structural': ['in_degree', 'out_degree', 'is_entry', 'is_exit', 'is_critical_node'],\n",
    "        'custom_patterns': ['loop_pattern', 'call_ret_sequence', 'api_call_sequence']\n",
    "    }\n",
    "    \n",
    "    total_valid_nodes = stats['total_valid_nodes']\n",
    "    \n",
    "    for inst, count in stats['instruction_types'].items():\n",
    "        if count / total_valid_nodes * 100 >= inst_threshold:\n",
    "            feature_model['instruction_types'].append(inst)\n",
    "    \n",
    "    for reg, count in stats['registers_used'].items():\n",
    "        if count / total_valid_nodes * 100 >= reg_threshold:\n",
    "            feature_model['registers'].append(reg)\n",
    "    \n",
    "    for api, count in stats['api_calls'].items():\n",
    "        if count >= api_min_count:\n",
    "            clean_api = re.sub(r'[^a-zA-Z0-9_]', '', api.lower())\n",
    "            if clean_api:\n",
    "                feature_model['apis'].append(clean_api)\n",
    "    \n",
    "    total_features = sum(len(category) for category in feature_model.values())\n",
    "    \n",
    "    print(f\"Modèle de features avancé créé avec {total_features} features au total:\")\n",
    "    for category, features in feature_model.items():\n",
    "        print(f\"  {category}: {len(features)} features\")\n",
    "    \n",
    "    return feature_model\n",
    "\n",
    "def extract_advanced_features(G, feature_model):\n",
    "    \"\"\"\n",
    "    Extrait des features avancées pour chaque nœud du graphe\n",
    "    avec gestion des dimensions inconsistantes\n",
    "    \"\"\"\n",
    "    node_features = {}\n",
    "   \n",
    "    max_dim = 0\n",
    "    for category in feature_model.values():\n",
    "        max_dim += len(category)\n",
    "   \n",
    "    centrality = nx.betweenness_centrality(G, k=min(100, len(G)))\n",
    "    degrees = dict(G.degree())\n",
    "    in_degrees = dict(G.in_degree())\n",
    "    out_degrees = dict(G.out_degree())\n",
    "   \n",
    "    entry_nodes = {n for n, d in G.in_degree() if d == 0}\n",
    "    exit_nodes = {n for n, d in G.out_degree() if d == 0}\n",
    "   \n",
    "    critical_nodes = {n for n, c in centrality.items() if c > 0.1}\n",
    "   \n",
    "    for node, attrs in G.nodes(data=True):\n",
    "        try:\n",
    "            features = np.zeros(max_dim, dtype=np.float32)\n",
    "            feature_idx = 0\n",
    "           \n",
    "            in_degree = in_degrees.get(node, 0)\n",
    "            out_degree = out_degrees.get(node, 0)\n",
    "            is_entry = 1 if node in entry_nodes else 0\n",
    "            is_exit = 1 if node in exit_nodes else 0\n",
    "            is_critical = 1 if node in critical_nodes else 0\n",
    "           \n",
    "            structural_features = [in_degree, out_degree, is_entry, is_exit, is_critical]\n",
    "            for i, feat in enumerate(structural_features):\n",
    "                if feature_idx + i < max_dim:\n",
    "                    features[feature_idx + i] = feat\n",
    "            feature_idx += len(structural_features)\n",
    "           \n",
    "            if 'label' in attrs:\n",
    "                label = attrs['label'].lower()\n",
    "               \n",
    "                feature_sets = [\n",
    "                    ('instruction_types', feature_model['instruction_types']),\n",
    "                    ('registers', feature_model['registers']),\n",
    "                    ('apis', feature_model['apis']),\n",
    "                ]\n",
    "               \n",
    "                for set_name, feature_set in feature_sets:\n",
    "                    if feature_idx + len(feature_set) <= max_dim:\n",
    "                        feature_idx += len(feature_set)\n",
    "           \n",
    "            features = features[:max_dim]\n",
    "           \n",
    "            node_features[node] = features\n",
    "           \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'extraction de features pour le nœud {node}: {str(e)}\")\n",
    "            node_features[node] = np.zeros(max_dim, dtype=np.float32)\n",
    "   \n",
    "    return node_features\n",
    "\n",
    "def calculate_graph_features(G, node_features):\n",
    "    \"\"\"\n",
    "    Calcule des features globales pour le graphe entier\n",
    "    \"\"\"\n",
    "    if not node_features:\n",
    "        return np.zeros(1, dtype=np.float32)\n",
    "    \n",
    "    avg_features = np.mean(list(node_features.values()), axis=0)\n",
    "    \n",
    "    graph_features = {\n",
    "        'n_nodes': len(G),\n",
    "        'n_edges': len(G.edges()),\n",
    "        'density': nx.density(G),\n",
    "        'avg_in_degree': sum(d for _, d in G.in_degree()) / max(1, len(G)),\n",
    "        'avg_out_degree': sum(d for _, d in G.out_degree()) / max(1, len(G)),\n",
    "        'max_in_degree': max((d for _, d in G.in_degree()), default=0),\n",
    "        'max_out_degree': max((d for _, d in G.out_degree()), default=0),\n",
    "        'n_entry_points': sum(1 for _, d in G.in_degree() if d == 0),\n",
    "        'n_exit_points': sum(1 for _, d in G.out_degree() if d == 0),\n",
    "        'avg_path_length': nx.average_shortest_path_length(G) if nx.is_strongly_connected(G) and len(G) > 1 else 0,\n",
    "        'n_connected_components': nx.number_weakly_connected_components(G),\n",
    "        'largest_component_size': max(len(c) for c in nx.weakly_connected_components(G)) if len(G) > 0 else 0,\n",
    "    }\n",
    "    \n",
    "    graph_features_vector = np.array(list(graph_features.values()), dtype=np.float32)\n",
    "    \n",
    "    combined_features = np.concatenate([avg_features, graph_features_vector])\n",
    "    \n",
    "    return combined_features\n",
    "\n",
    "def convert_to_pytorch_geometric(G, node_features, label=None):\n",
    "    \"\"\"\n",
    "    Convertit un graphe en format PyTorch Geometric avec gestion des dimensions inconsistantes\n",
    "    \"\"\"\n",
    "    node_mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "   \n",
    "    edge_index = []\n",
    "    for source, target in G.edges():\n",
    "        if source in node_mapping and target in node_mapping:\n",
    "            edge_index.append([node_mapping[source], node_mapping[target]])\n",
    "   \n",
    "    if not edge_index:\n",
    "        edge_index = np.zeros((2, 0), dtype=np.int64)\n",
    "    else:\n",
    "        edge_index = np.array(edge_index, dtype=np.int64).T\n",
    "   \n",
    "    feature_dims = [feat.shape[0] for feat in node_features.values()]\n",
    "    if len(set(feature_dims)) > 1:\n",
    "        most_common_dim = max(set(feature_dims), key=feature_dims.count)\n",
    "       \n",
    "        for node in node_features:\n",
    "            if node_features[node].shape[0] != most_common_dim:\n",
    "                if node_features[node].shape[0] < most_common_dim:\n",
    "                    node_features[node] = np.pad(\n",
    "                        node_features[node],\n",
    "                        (0, most_common_dim - node_features[node].shape[0]),\n",
    "                        'constant'\n",
    "                    )\n",
    "                else:\n",
    "                    node_features[node] = node_features[node][:most_common_dim]\n",
    "   \n",
    "    feature_dim = list(node_features.values())[0].shape[0]\n",
    "    x = np.zeros((len(node_mapping), feature_dim), dtype=np.float32)\n",
    "    for node, idx in node_mapping.items():\n",
    "        if node in node_features:\n",
    "            x[idx] = node_features[node]\n",
    "   \n",
    "    y = None\n",
    "    if label is not None:\n",
    "        y = np.array(label, dtype=np.float32)\n",
    "   \n",
    "    data = Data(\n",
    "        x=torch.tensor(x, dtype=torch.float),\n",
    "        edge_index=torch.tensor(edge_index, dtype=torch.long),\n",
    "        y=torch.tensor(y, dtype=torch.float) if y is not None else None\n",
    "    )\n",
    "   \n",
    "    return data\n",
    "\n",
    "class CFGDataset(Dataset):\n",
    "    def __init__(self, digraph_dir, labels_path, feature_model, transform=None, pre_transform=None, root=None):\n",
    "        \"\"\"\n",
    "        Dataset pour les graphes de flux de contrôle\n",
    "        \"\"\"\n",
    "        self.digraph_dir = digraph_dir\n",
    "        self.labels_path = labels_path\n",
    "        self.feature_model = feature_model\n",
    "        \n",
    "        self.labels_df = pd.read_excel(labels_path)\n",
    "        \n",
    "        self.behaviors = [col for col in self.labels_df.columns \n",
    "                         if col not in ['name', 'num_antivirus_malicious', 'first_submission_date', 'suggested_threat_label']]\n",
    "        \n",
    "        self.files = [f for f in os.listdir(digraph_dir) if f.endswith('.json') and \n",
    "                      os.path.splitext(f)[0] in self.labels_df['name'].values]\n",
    "        \n",
    "        self.file_labels = {}\n",
    "        for file in self.files:\n",
    "            file_id = os.path.splitext(file)[0]\n",
    "            if file_id in self.labels_df['name'].values:\n",
    "                self.file_labels[file] = self.labels_df.loc[self.labels_df['name'] == file_id, self.behaviors].values[0]\n",
    "        \n",
    "        super(CFGDataset, self).__init__(root, transform, pre_transform)\n",
    "    \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return self.files\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [f'data_{i}.pt' for i in range(len(self.files))]\n",
    "    \n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        for i, file in enumerate(tqdm(self.files, desc=\"Traitement des graphes\")):\n",
    "            G = load_digraph(os.path.join(self.digraph_dir, file))\n",
    "            if G is None or len(G) == 0:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                node_features = extract_advanced_features(G, self.feature_model)\n",
    "                \n",
    "                labels = self.file_labels.get(file, np.zeros(len(self.behaviors)))\n",
    "                \n",
    "                data = convert_to_pytorch_geometric(G, node_features, labels)\n",
    "                \n",
    "                data.file_name = file\n",
    "                \n",
    "                torch.save(data, os.path.join(self.processed_dir, f'data_{i}.pt'))\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors du traitement de {file}: {str(e)}\")\n",
    "    \n",
    "    def len(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        data = torch.load(os.path.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data\n",
    "\n",
    "def prepare_cfg_dataset(digraph_dir, labels_path, output_dir, sample_size=5000):\n",
    "    \"\"\"\n",
    "    Prépare un dataset sur des graphes CFG\n",
    "    \"\"\"\n",
    "    print(f\"Analyse des statistiques sur un échantillon de {sample_size} fichiers...\")\n",
    "    stats = analyze_instruction_stats(digraph_dir, max_files=sample_size)\n",
    "    \n",
    "    print(\"\\nCréation du modèle de features...\")\n",
    "    feature_model = create_advanced_feature_model(stats, inst_threshold=1.0, reg_threshold=1.0, api_min_count=10)\n",
    "\n",
    "    with open(os.path.join(output_dir, 'feature_model.pkl'), 'wb') as f:\n",
    "        pickle.dump(feature_model, f)\n",
    "    \n",
    "    print(\"\\nCréation du dataset...\")\n",
    "    dataset = CFGDataset(\n",
    "        digraph_dir=digraph_dir,\n",
    "        labels_path=labels_path,\n",
    "        feature_model=feature_model,\n",
    "        root=output_dir\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset créé avec {len(dataset)} graphes\")\n",
    "    \n",
    "\n",
    "    with open(os.path.join(output_dir, 'stats.json'), 'w') as f:\n",
    "        stats_json = {k: (dict(v) if isinstance(v, Counter) else v) for k, v in stats.items()}\n",
    "        json.dump(stats_json, f, indent=2)\n",
    "    \n",
    "    return dataset, feature_model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Préparation du dataset\")\n",
    "    dataset, feature_model = prepare_cfg_dataset(\n",
    "        digraph_dir=DIGRAPH_DIR,\n",
    "        labels_path=LABELS_PATH,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        sample_size=SAMPLE_SIZE\n",
    "    )\n",
    "    print(\"Préparation terminée!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "pt_file_path = \"pt_output/data_2.pt\"\n",
    "data = torch.load(pt_file_path, weights_only=False)\n",
    "\n",
    "print(\"Attributs internes (data.__dict__):\")\n",
    "for key, value in data.__dict__.items():\n",
    "    if hasattr(value, 'shape'):\n",
    "        print(f\"{key}: shape = {value.shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data._store['file_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Ce script traite par batch les fichiers `.pt` contenant des graphes encodés (format `torch_geometric.data.Data`) afin d’en extraire des statistiques globales, notamment :\n",
    "- Des métriques sur les embeddings des nœuds (`x_mean`, `x_var`, `x_l2`, etc.),\n",
    "- Des propriétés structurelles (`in/out degree`, densité, présence de boucles, etc.).\n",
    "\n",
    "Les résultats sont stockés en plusieurs fichiers CSV (`_features_batch_XX.csv`), avec détection automatique des fichiers déjà extraits pour éviter les doublons.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from torch_geometric.utils import to_networkx, contains_self_loops\n",
    "\n",
    "pt_folder = \"pt_output\"\n",
    "csv_path = \"data/test.csv\"\n",
    "output_csv_base = \"data/test_features_batch\"\n",
    "batch_size = 2500\n",
    "\n",
    "existing_graph_ids = set()\n",
    "existing_csvs = glob(f\"{output_csv_base}_*.csv\")\n",
    "for csv_file in existing_csvs:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, usecols=[\"graph_id\"])\n",
    "        existing_graph_ids.update(df[\"graph_id\"].dropna().astype(str).tolist())\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur en lisant {csv_file} : {e}\")\n",
    "\n",
    "print(f\"Graphs déjà traités : {len(existing_graph_ids)}\")\n",
    "\n",
    "all_files = sorted([f for f in os.listdir(pt_folder) if f.endswith(\".pt\")])\n",
    "num_batches = (len(all_files) + batch_size - 1) // batch_size\n",
    "\n",
    "for batch_idx in range(num_batches):\n",
    "    output_csv_path = f\"{output_csv_base}_{batch_idx + 1}.csv\"\n",
    "    if os.path.exists(output_csv_path):\n",
    "        print(f\"Batch {batch_idx + 1} déjà existant, on skip...\")\n",
    "        continue\n",
    "\n",
    "    rows = []\n",
    "    batch_files = all_files[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
    "    print(f\"\\n Traitement batch {batch_idx + 1}/{num_batches} ({len(batch_files)} fichiers)...\")\n",
    "\n",
    "    for fname in tqdm(batch_files):\n",
    "        try:\n",
    "            data = torch.load(os.path.join(pt_folder, fname), weights_only=False)\n",
    "            file_name = getattr(data, 'file_name', None)\n",
    "            if file_name is None:\n",
    "                continue\n",
    "\n",
    "            graph_id = file_name.replace(\".json\", \"\")\n",
    "            if graph_id in existing_graph_ids:\n",
    "                continue \n",
    "\n",
    "            x = data.x\n",
    "            x_l2 = x.norm(p=2, dim=1)\n",
    "\n",
    "            G_nx = to_networkx(data, to_undirected=False)\n",
    "            in_deg_mean = np.mean([d for _, d in G_nx.in_degree()])\n",
    "            out_deg_mean = np.mean([d for _, d in G_nx.out_degree()])\n",
    "            num_components = nx.number_strongly_connected_components(G_nx)\n",
    "            density = nx.density(G_nx)\n",
    "            has_loops = contains_self_loops(data.edge_index)\n",
    "\n",
    "            row = {\n",
    "                \"graph_id\": graph_id,\n",
    "                \"feature_dim\": x.shape[1],\n",
    "                \"x_mean_all\": float(x.mean().item()),\n",
    "                \"x_var_all\": float(x.var().item()),\n",
    "                \"x_abs_mean\": float(x.abs().mean().item()),\n",
    "                \"x_max\": float(x.max().item()),\n",
    "                \"x_min\": float(x.min().item()),\n",
    "                \"x_l2_mean\": float(x_l2.mean().item()),\n",
    "                \"x_l2_max\": float(x_l2.max().item()),\n",
    "                \"x_l2_min\": float(x_l2.min().item()),\n",
    "                \"x_l2_std\": float(x_l2.std().item()),\n",
    "                \"x_l2_ratio_max_mean\": float(x_l2.max().item()) / (x_l2.mean().item() + 1e-6),\n",
    "                \"num_nodes_l2_above_1\": int((x_l2 > 1.0).sum().item()),\n",
    "                \"num_nodes_l2_below_0.1\": int((x_l2 < 0.1).sum().item()),\n",
    "                \"num_nodes\": data.num_nodes,\n",
    "                \"num_edges\": data.num_edges,\n",
    "                \"in_degree_mean\": in_deg_mean,\n",
    "                \"out_degree_mean\": out_deg_mean,\n",
    "                \"has_self_loops\": int(has_loops),\n",
    "                \"num_connected_components\": num_components,\n",
    "                \"density\": density,\n",
    "            }\n",
    "            rows.append(row)\n",
    "            existing_graph_ids.add(graph_id) \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur avec {fname} : {e}\")\n",
    "\n",
    "    features_df = pd.DataFrame(rows)\n",
    "    features_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Batch {batch_idx + 1} sauvegardé dans {output_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
