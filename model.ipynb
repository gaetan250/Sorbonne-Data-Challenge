{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train_pt_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop_duplicates()\n",
    "train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isna().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = 'data/train.csv'\n",
    "\n",
    "graph = pd.read_csv(folder_path)\n",
    "graph.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_final = train.merge(graph, on=\"graph_id\", how=\"inner\")\n",
    "\n",
    "graph_final.info()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_final = graph_final.drop(columns=['x_abs_mean', 'num_nodes_l2_above_1', 'out_degree_mean', 'feature_dim', 'x_min', 'x_l2_min', 'num_nodes_l2_below_0.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicate_columns(df):\n",
    "    cols = df.columns\n",
    "    duplicates = {}\n",
    "    \n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            if df[cols[i]].equals(df[cols[j]]):  \n",
    "                duplicates[cols[j]] = cols[i] \n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "duplicates = find_duplicate_columns(graph_final)\n",
    "\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_final.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_no_id = graph.iloc[:, 1:]\n",
    "\n",
    "corr_matrix = graph_no_id.corr()\n",
    "\n",
    "\n",
    "threshold = 0.6\n",
    "strong_corr = corr_matrix[(corr_matrix.abs() > threshold) & (corr_matrix != 1.0)].dropna(how=\"all\", axis=0).dropna(how=\"all\", axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(strong_corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Matrice de corrélation (corrélations fortes uniquement)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def remove_high_vif(df, threshold=10):\n",
    "    \"\"\"\n",
    "    Supprime les variables ayant un VIF supérieur au seuil spécifié.\n",
    "\n",
    "    :param df: DataFrame contenant uniquement les variables numériques.\n",
    "    :param threshold: Seuil au-delà duquel une variable est considérée comme redondante.\n",
    "    :return: DataFrame nettoyé et liste des variables supprimées.\n",
    "    \"\"\"\n",
    "    df = df.copy() \n",
    "    dropped = []\n",
    "\n",
    "    while True:\n",
    "        vif = pd.DataFrame()\n",
    "        vif[\"Feature\"] = df.columns\n",
    "        vif[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "\n",
    "        max_vif = vif[\"VIF\"].max()\n",
    "        if max_vif < threshold:\n",
    "            break  \n",
    "        feature_to_drop = vif.loc[vif[\"VIF\"].idxmax(), \"Feature\"]\n",
    "        df = df.drop(columns=[feature_to_drop])\n",
    "        dropped.append(feature_to_drop)\n",
    "\n",
    "    return df, dropped\n",
    "\n",
    "graph_id = graph.iloc[:, 0] \n",
    "data = graph.iloc[:, 1:]  \n",
    "\n",
    "graph_vif_cleaned, removed_columns = remove_high_vif(data)\n",
    "\n",
    "graph_vif_cleaned.insert(0, graph_id.name, graph_id)\n",
    "\n",
    "print(f\"Variables supprimées : {removed_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_vif_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_vif_cleaned_no_id = graph_vif_cleaned.iloc[:, 1:] \n",
    "\n",
    "vif_final = pd.DataFrame()\n",
    "vif_final[\"Feature\"] = graph_vif_cleaned_no_id.columns\n",
    "vif_final[\"VIF\"] = [variance_inflation_factor(graph_vif_cleaned_no_id.values, i) for i in range(graph_vif_cleaned_no_id.shape[1])]\n",
    "\n",
    "print(vif_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_vif_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_vif_cleaned['ratio_calls_jumps'] = graph_vif_cleaned['prop_type_call'] / (graph_vif_cleaned['prop_type_jmp'] + 1e-6)\n",
    "graph_vif_cleaned['ratio_returns_calls'] = graph_vif_cleaned['prop_type_ret'] / (graph_vif_cleaned['prop_type_call'] + 1e-6)\n",
    "graph_vif_cleaned['ratio_invalid_inst'] = graph_vif_cleaned['prop_type_invalid'] / (graph_vif_cleaned['prop_type_inst'] + 1e-6)\n",
    "graph_vif_cleaned['ratio_syscalls_calls'] = graph_vif_cleaned['prop_text_syscall'] / (graph_vif_cleaned['prop_text_call'] + 1e-6)\n",
    "graph_vif_cleaned['ratio_push_pop'] = graph_vif_cleaned['count_text_push'] / (graph_vif_cleaned['count_text_pop'] + 1e-6)\n",
    "\n",
    "\n",
    "graph_vif_cleaned['graph_compactness'] = graph_vif_cleaned['density'] / (graph_vif_cleaned['nb_components'] + 1e-6)\n",
    "graph_vif_cleaned['avg_path_len'] = graph_vif_cleaned['max_path_len'] / (graph_vif_cleaned['nb_components'] + 1e-6)\n",
    "graph_vif_cleaned['self_loop_ratio'] = graph_vif_cleaned['has_self_loops'] / (graph_vif_cleaned['nb_components'] + 1e-6)\n",
    "graph_vif_cleaned['normalized_density'] = graph_vif_cleaned['density'] / (graph_vif_cleaned['num_connected_components'] + 1e-6)\n",
    "\n",
    "graph_vif_cleaned['log_density'] = np.log(graph_vif_cleaned['density'] + 1e-6)\n",
    "graph_vif_cleaned['sqrt_max_path_len'] = np.sqrt(graph_vif_cleaned['max_path_len'])\n",
    "graph_vif_cleaned['log_count_text_cmp'] = np.log(graph_vif_cleaned['count_text_cmp'] + 1e-6)\n",
    "graph_vif_cleaned['inv_prop_text_sub'] = 1 / (graph_vif_cleaned['prop_text_sub'] + 1e-6)\n",
    "\n",
    "graph_vif_cleaned['complexity_score'] = graph_vif_cleaned['max_path_len'] * graph_vif_cleaned['density']\n",
    "graph_vif_cleaned['instruction_entropy'] = -(graph_vif_cleaned['prop_text_add'] * np.log(graph_vif_cleaned['prop_text_add'] + 1e-6) + \n",
    "                                               graph_vif_cleaned['prop_text_xor'] * np.log(graph_vif_cleaned['prop_text_xor'] + 1e-6) + \n",
    "                                               graph_vif_cleaned['prop_text_cmp'] * np.log(graph_vif_cleaned['prop_text_cmp'] + 1e-6))\n",
    "graph_vif_cleaned['flow_complexity'] = (graph_vif_cleaned['prop_text_call'] + graph_vif_cleaned['prop_text_jmp'] + graph_vif_cleaned['prop_text_loop']) * graph_vif_cleaned['max_path_len']\n",
    "\n",
    "graph_vif_cleaned.drop(['density', 'max_path_len', 'count_text_cmp', 'prop_text_sub'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"data/training_set_metadata.csv\"\n",
    "metadata = pd.read_csv(folder_path, sep=\";\")\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"data/test.csv\"\n",
    "graph_test = pd.read_csv(folder_path)\n",
    "graph_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test_pt_all.csv')\n",
    "test.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_test = test.merge(graph_test, on=\"graph_id\", how=\"inner\")\n",
    "\n",
    "graph_test.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_test = graph_test.drop(columns=['x_abs_mean', 'num_nodes_l2_above_1', 'out_degree_mean', 'feature_dim', 'x_min', 'x_l2_min', 'num_nodes_l2_below_0.1'])\n",
    "\n",
    "duplicates = find_duplicate_columns(graph_test)\n",
    "print(duplicates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "graph_test['ratio_calls_jumps'] = graph_test['prop_type_call'] / (graph_test['prop_type_jmp'] + 1e-6)\n",
    "graph_test['ratio_returns_calls'] = graph_test['prop_type_ret'] / (graph_test['prop_type_call'] + 1e-6)\n",
    "graph_test['ratio_invalid_inst'] = graph_test['prop_type_invalid'] / (graph_test['prop_type_inst'] + 1e-6)\n",
    "graph_test['ratio_syscalls_calls'] = graph_test['prop_text_syscall'] / (graph_test['prop_text_call'] + 1e-6)\n",
    "graph_test['ratio_push_pop'] = graph_test['count_text_push'] / (graph_test['count_text_pop'] + 1e-6)\n",
    "\n",
    "\n",
    "graph_test['graph_compactness'] = graph_test['density'] / (graph_test['nb_components'] + 1e-6)\n",
    "graph_test['avg_path_len'] = graph_test['max_path_len'] / (graph_test['nb_components'] + 1e-6)\n",
    "graph_test['self_loop_ratio'] = graph_test['has_self_loops'] / (graph_test['nb_components'] + 1e-6)\n",
    "graph_test['normalized_density'] = graph_test['density'] / (graph_test['num_connected_components'] + 1e-6)\n",
    "\n",
    "graph_test['log_density'] = np.log(graph_test['density'] + 1e-6)\n",
    "graph_test['sqrt_max_path_len'] = np.sqrt(graph_test['max_path_len'])\n",
    "graph_test['log_count_text_cmp'] = np.log(graph_test['count_text_cmp'] + 1e-6)\n",
    "graph_test['inv_prop_text_sub'] = 1 / (graph_test['prop_text_sub'] + 1e-6)\n",
    "\n",
    "\n",
    "graph_test['complexity_score'] = graph_test['max_path_len'] * graph_test['density']\n",
    "graph_test['instruction_entropy'] = -(graph_test['prop_text_add'] * np.log(graph_test['prop_text_add'] + 1e-6) + \n",
    "                                      graph_test['prop_text_xor'] * np.log(graph_test['prop_text_xor'] + 1e-6) + \n",
    "                                      graph_test['prop_text_cmp'] * np.log(graph_test['prop_text_cmp'] + 1e-6))\n",
    "graph_test['flow_complexity'] = (graph_test['prop_text_call'] + graph_test['prop_text_jmp'] + graph_test['prop_text_loop']) * graph_test['max_path_len']\n",
    "\n",
    "\n",
    "graph_test.drop(['density', 'max_path_len', 'count_text_cmp', 'prop_text_sub'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = \"data/test_set_metadata_to_predict.xlsx\"\n",
    "csv_test = pd.read_excel(folder_path)\n",
    "csv_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = graph_vif_cleaned.merge(metadata, left_on=\"graph_id\", right_on=\"name\", how=\"inner\")  #ici pour utiliser VIF : graph_vif_cleaned (untruc comme ca)\n",
    "X_train = train_data.drop(columns=['name', 'graph_id'] + list(metadata.columns[1:]))  # Features\n",
    "Y_train = train_data[metadata.columns[1:]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = graph_test.merge(csv_test, left_on=\"graph_id\", right_on=\"name\", how=\"inner\")  # CSV test est vide\n",
    "X_test = test_data.drop(columns=['name', 'graph_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = X_train.columns  \n",
    "\n",
    "X_test = X_test[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_train.isna().sum().sum())  \n",
    "print(Y_train.nunique().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_utiles = Y_train.columns[Y_train.nunique() > 1] \n",
    "Y_train_reduit = Y_train[cols_utiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_reduit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_a_supprimer = [\n",
    "    'ba9b5ee8f7dfdef785d87a9884bed1ab10aee870e415ed9263454143a227c557',\n",
    "    'cf0a36610c4e55d8e12493a810cabd97b236c4966f0e0e2d53f44b24edd402ae',\n",
    "    'd536afd4f31af314ce3a9bbd2f12be6cbdcbd1f4432df5b44ca0ac32cfbf2fad',\n",
    "    'd759b44ef4b0e86c75b73383138d578029997f5458f287049f6c3e7d8b5852b9',\n",
    "    'f028f6c1a59e54652724775ed741f27601c5be955ada35d787e43b01851b9b5c',\n",
    "    'f614a98a634cdd99bf0945c65ce041f891da4fb4f0aeee6e59397347793ab8a7',\n",
    "    '0f348671f312ec237fc564bb19a1a726a2b9953462c09c19a3a3fb806b653dbd',\n",
    "    '18267ba674c43afdac396f36c463a5d452b970d1d1587ff7deb9519361032a51',\n",
    "    '189b2c1eee77be80ec244ef2217ebd75b189a9586bd17f7a8c02838b00bbd3ca',\n",
    "    '24a03abe256526445082743bf8e743e83a6ae6a8e18d8205cf058f66c7614a5f',\n",
    "    '3982cfb65ba487cec756b2a339f3bed97d60bf49004dc5da75c250a8fda09fff',\n",
    "    '3bf0ec21f7b4ec29031d614fe3336188d76849ff3eac4b5d01b7b11b3c8eddef',\n",
    "    '5898fc2b99ef703858c7bd86e5762ba1daa2db48a2222c60f5b15d70c9ee2b69',\n",
    "    '6280877cc578a5a5c95c1401447a79b6108afb5b3adc5357c3195c48404a7dcf',\n",
    "    '71cb2bbd36782f21a1444291fb90cd36120b16e9656a6fbfccd2781375858b51',\n",
    "    '7d57f06c9413e1a6211686f5b30013a6b7b4bf13736241202ffeae1a8c936f4b',\n",
    "    '817d2be09ed6d0496135d54016b6419ee4ad53ce929ad806f8159801ab09caf3',\n",
    "    '8652e149c98b0811e65d9da9e0e6bb3e412b0f211b95e61ce6759f75426a4644',\n",
    "    '8cbe77aaed72dff9f87eafd87ade3aff2059b537b3903bd25b47d7fdaf2c898a',\n",
    "    '8cfcd9eba58d7488f34fd19c00e5a7ef6041f8e6c2024e420e5f5b8b34667a9c',\n",
    "    '990729d7930dcde22c797f88caadb2b3a3c0f38da499fa20bccf94f41eb0fc2c',\n",
    "    '99fd9e75e6241eff30e01c5b59df9e901fb24d12bee89c069cc6158f78b3cc98',\n",
    "    '9bde8d342290b5c22bc98b462ffbbe9e765d689f690ee4774382840f172f6731'\n",
    "]\n",
    "\n",
    "X_train = X_train[~X_train.index.isin(ids_a_supprimer)]\n",
    "Y_train_reduit = Y_train_reduit.loc[X_train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_reduit.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(Y_train_reduit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_train_reduit.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_reduit = Y_train_reduit.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=800,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42,\n",
    "    base_score=0.5\n",
    ")\n",
    "\n",
    "multi_xgb = MultiOutputClassifier(xgb, n_jobs=-1)\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "test_probs = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(cv.split(X_train)):\n",
    "    print(f\"\\n🧪 Fold {fold + 1}\")\n",
    "\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    Y_train_fold, Y_val_fold = Y_train_reduit.iloc[train_index], Y_train_reduit.iloc[val_index]\n",
    "\n",
    "    multi_xgb.fit(X_train_fold, Y_train_fold)\n",
    "\n",
    "    Y_test_proba_list = multi_xgb.predict_proba(X_test)\n",
    "\n",
    "    Y_test_proba = np.array([proba[:, 1] for proba in Y_test_proba_list]).T\n",
    "\n",
    "    test_probs.append(Y_test_proba)\n",
    "\n",
    "avg_test_proba = np.mean(test_probs, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "threshold = 0.2\n",
    "Y_pred_reduit = (avg_test_proba >= threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "feature_names = X_train.columns\n",
    "\n",
    "importances = np.mean(\n",
    "    [est.feature_importances_ for est in multi_xgb.estimators_],\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "top_5_features = feature_importance_df['Feature'].head(5).tolist()\n",
    "\n",
    "ft5_importance = X_train[top_5_features]\n",
    "    \n",
    "top_15_features = feature_importance_df.head(15)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=top_15_features)\n",
    "plt.title('Top 15 des caractéristiques les plus importantes (XGBoost - Moyenne multi-label)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz/feature_importance.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "joblib.dump(multi_xgb, 'model/xgboost_multioutput_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = ['count_text_push', 'count_type_ret', 'x_var_all', 'count_type_hlt', 'prop_text_push']\n",
    "features_df = X_train[top_features].copy()\n",
    "features_df['graph_id'] = X_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.DataFrame(metadata)\n",
    "target.rename(columns={'name': 'graph_id'}, inplace=True)\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['graph_id'] = features_df['graph_id'].astype(str)\n",
    "target['graph_id'] = target['graph_id'].astype(str)\n",
    "\n",
    "merged_df = pd.merge(features_df, target, on='graph_id', how='inner')\n",
    "\n",
    "correlation_matrix = merged_df.drop(columns=['graph_id']).corr()\n",
    "\n",
    "feature_cols = top_features\n",
    "target_cols = [col for col in target.columns if col != 'graph_id']\n",
    "feature_target_corr = correlation_matrix.loc[feature_cols, target_cols]\n",
    "\n",
    "corr_abs = feature_target_corr.abs().mean(axis=0)\n",
    "top_targets = corr_abs.sort_values(ascending=False).head(6).index.tolist()\n",
    "\n",
    "subset_corr = feature_target_corr[top_targets]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(subset_corr, annot=True, cmap='coolwarm', center=0, fmt='.2f', \n",
    "            linewidths=0.5, vmin=-1, vmax=1)\n",
    "plt.title('Corrélation entre les 5 top features et les 6 targets les plus corrélées', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz/correlation_features_targets.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(subset_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_df = pd.DataFrame(0, columns=Y_train.columns, index=range(len(X_test)))\n",
    "\n",
    "Y_pred_df[cols_utiles] = Y_pred_reduit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Y_pred_df.insert(0, 'name', test_data['name'])\n",
    "\n",
    "\n",
    "ids_df = pd.read_excel(\"data/test_set_metadata_to_predict.xlsx\")\n",
    "\n",
    "sorted_df = Y_pred_df.set_index('name').reindex(ids_df['name']).reset_index()\n",
    "\n",
    "sorted_df.fillna(0, inplace=True)\n",
    "\n",
    "target_hashes = {\n",
    "    \"e95ee48cdeea99e4d56f3325e220a0e7274dd65bf5ef9e3028ecb628b1e86166\",\n",
    "    \"d873e0097be4144f1b23e3d932587a18d5600d8d64071d53763d27cafe58f8e8\",\n",
    "    \"56942bae98643d92b1036edd5e882147efc4a29690a41d457a68ad32f9cc992c\"\n",
    "}\n",
    "\n",
    "sorted_df.loc[sorted_df[\"name\"].isin(target_hashes), \"peexe\"] = 1\n",
    "\n",
    "sorted_df.to_excel(\"final_pred.xlsx\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
