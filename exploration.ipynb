{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce script lit les deux premiers fichiers `.json` représentant des graphes DOT, identifie les types de nœuds (entry, call, jmp...), trace un sous-graphe interactif avec flèches directionnelles et couleurs, et génère une visualisation HTML par graphe.  \n",
    "Chaque graphe est stylisé avec une légende, un fond clair, et les flèches indiquent le flot de contrôle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "import webbrowser\n",
    "\n",
    "input_dir = \"folder_test_set\"\n",
    "output_dir = \"graph_viz\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "COLOR_MAP = {\n",
    "    \"ENTRY\": \"black\",\n",
    "    \"CALL\": \"orange\",\n",
    "    \"JMP\": \"skyblue\",\n",
    "    \"JCC\": \"violet\",\n",
    "    \"RET\": \"deepskyblue\",\n",
    "    \"OTHER\": \"gray\"\n",
    "}\n",
    "\n",
    "def parse_dot_to_graph(dot_str):\n",
    "    G = nx.DiGraph()\n",
    "    lines = dot_str.strip().split(\"\\n\")\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if \"->\" in line:\n",
    "            match = re.match(r'\"([^\"]+)\"\\s*->\\s*\"([^\"]+)\"', line)\n",
    "            if match:\n",
    "                src, dst = match.groups()\n",
    "                G.add_edge(src, dst)\n",
    "        elif \"[\" in line and \"label =\" in line:\n",
    "            match = re.match(r'\"([^\"]+)\"\\s*\\[label = \"(.*?)\"\\]', line)\n",
    "            if match:\n",
    "                node_id, label = match.groups()\n",
    "                parts = label.split(\" : \")\n",
    "                instr_text = parts[1] if len(parts) > 1 else \"\"\n",
    "                G.add_node(node_id, text=instr_text.upper())\n",
    "    return G\n",
    "\n",
    "def get_instr_class(instr, node, entry_node):\n",
    "    if node == entry_node:\n",
    "        return \"ENTRY\"\n",
    "    if \"CALL\" in instr:\n",
    "        return \"CALL\"\n",
    "    if \"JMP\" in instr and not \"JCC\" in instr:\n",
    "        return \"JMP\"\n",
    "    if \"JCC\" in instr:\n",
    "        return \"JCC\"\n",
    "    if \"RET\" in instr:\n",
    "        return \"RET\"\n",
    "    return \"OTHER\"\n",
    "\n",
    "def find_entry_node(G):\n",
    "    for node, data in G.nodes(data=True):\n",
    "        if \"ENTRY\" in data.get(\"text\", \"\"):\n",
    "            return node\n",
    "    entry_candidates = [n for n in G.nodes if G.in_degree(n) == 0]\n",
    "    if entry_candidates:\n",
    "        return entry_candidates[0]\n",
    "    return list(G.nodes)[0]\n",
    "\n",
    "def graph_to_html(G, graph_id, max_nodes=50):\n",
    "    nodes = list(G.nodes())[:max_nodes]\n",
    "    H = G.subgraph(nodes).copy()\n",
    "    pos = nx.spring_layout(H, seed=42)\n",
    "\n",
    "    entry_node = find_entry_node(H)\n",
    "\n",
    "    node_x, node_y, node_color, node_class, node_hover = [], [], [], [], []\n",
    "    for node in H.nodes():\n",
    "        x, y = pos[node]\n",
    "        instr = H.nodes[node].get(\"text\", \"\")\n",
    "        cls = get_instr_class(instr, node, entry_node)\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        node_color.append(COLOR_MAP[cls])\n",
    "        node_class.append(cls)\n",
    "        hover_label = \"ENTRY\" if node == entry_node else instr\n",
    "        node_hover.append(hover_label)\n",
    "\n",
    "    traces = []\n",
    "    for cls in COLOR_MAP:\n",
    "        indices = [i for i, c in enumerate(node_class) if c == cls]\n",
    "        if indices:\n",
    "            traces.append(go.Scatter(\n",
    "                x=[node_x[i] for i in indices],\n",
    "                y=[node_y[i] for i in indices],\n",
    "                mode='markers',\n",
    "                hovertext=[node_hover[i] for i in indices],\n",
    "                hoverinfo='text',\n",
    "                marker=dict(color=COLOR_MAP[cls], size=12, line=dict(width=1, color='black')),\n",
    "                name=cls\n",
    "            ))\n",
    "\n",
    "    annotations = []\n",
    "    for src, dst in H.edges():\n",
    "        if src in pos and dst in pos:\n",
    "            x0, y0 = pos[src]\n",
    "            x1, y1 = pos[dst]\n",
    "            annotations.append(dict(\n",
    "                ax=x0, ay=y0,\n",
    "                x=x1, y=y1,\n",
    "                xref='x', yref='y',\n",
    "                axref='x', ayref='y',\n",
    "                showarrow=True,\n",
    "                arrowhead=3,\n",
    "                arrowsize=2,\n",
    "                arrowwidth=1.5,\n",
    "                arrowcolor='gray',\n",
    "                opacity=0.7\n",
    "            ))\n",
    "\n",
    "    fig = go.Figure(data=traces,\n",
    "                   layout=go.Layout(\n",
    "                       title=f\"Graphe interactif - {graph_id}\",\n",
    "                       showlegend=True,\n",
    "                       annotations=annotations,\n",
    "                       hovermode='closest',\n",
    "                       margin=dict(b=20, l=5, r=5, t=40),\n",
    "                       xaxis=dict(showgrid=False, zeroline=False, visible=False),\n",
    "                       yaxis=dict(showgrid=False, zeroline=False, visible=False),\n",
    "                       plot_bgcolor='white',\n",
    "                       legend=dict(\n",
    "                           x=1.01,\n",
    "                           y=1,\n",
    "                           bgcolor='rgba(255,255,255,0.6)',\n",
    "                           bordercolor='gray',\n",
    "                           borderwidth=1\n",
    "                       )\n",
    "                   ))\n",
    "\n",
    "    output_path = os.path.abspath(os.path.join(output_dir, f\"{graph_id}_with_arrows.html\"))\n",
    "    fig.write_html(output_path)\n",
    "    webbrowser.open(f\"file://{output_path}\")\n",
    "\n",
    "json_files = sorted([f for f in os.listdir(input_dir) if f.endswith(\".json\")])\n",
    "for file in json_files[:2]:\n",
    "    graph_id = file.replace(\".json\", \"\")\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        dot_str = f.read()\n",
    "    G = parse_dot_to_graph(dot_str)\n",
    "    if G.number_of_nodes() > 0:\n",
    "        graph_to_html(G, graph_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Ces deux blocs de code analysent les labels les plus fréquemment activés. Le premier lit le fichier `final_pred.xlsx` pour afficher les 20 comportements les plus prédits par notre modèle sur le jeu de test. Le second fait la même analyse sur le fichier d'entraînement `training_set_metadata.csv`, basé sur les labels réels. Cela permet de comparer la distribution des prédictions aux véritables fréquences des comportements observés, et d’identifier d’éventuels biais ou sur-apprentissages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "df = pd.read_excel(\"final_pred.xlsx\")\n",
    "\n",
    "df_labels = df.drop(columns=[\"name\"])\n",
    "\n",
    "label_counts = df_labels.sum().sort_values(ascending=False)\n",
    "\n",
    "top_labels = label_counts.head(20)\n",
    "\n",
    "fig = px.bar(\n",
    "    x=top_labels.index,\n",
    "    y=top_labels.values,\n",
    "    labels={'x': 'Label', 'y': 'Nombre de prédictions positives'},\n",
    "    title='Top 20 des labels les plus prédits'\n",
    ")\n",
    "fig.update_layout(xaxis_tickangle=-45)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "df = pd.read_csv(\"data/training_set_metadata.csv\",sep=\";\")\n",
    "\n",
    "df_labels = df.drop(columns=[\"name\"])\n",
    "\n",
    "label_counts = df_labels.sum().sort_values(ascending=False)\n",
    "\n",
    "top_labels = label_counts.head(20)\n",
    "\n",
    "fig = px.bar(\n",
    "    x=top_labels.index,\n",
    "    y=top_labels.values,\n",
    "    labels={'x': 'Label', 'y': 'Nombre de prédictions positives'},\n",
    "    title='Top 20 des labels les plus prédits '\n",
    ")\n",
    "fig.update_layout(xaxis_tickangle=-45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Ce script permet de visualiser la distribution de chaque feature numérique dans le fichier `train.csv`. Pour chaque variable, un histogramme avec courbe de densité (KDE) est généré à l’aide de Seaborn. Cela permet de détecter d’éventuelles asymétries, valeurs extrêmes ou distributions atypiques. Une version avec affichage direct par figure est utilisée pour faciliter l’exploration interactive dans un notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "cols_per_fig = 4\n",
    "num_plots = len(num_cols)\n",
    "num_rows = math.ceil(num_plots / cols_per_fig)\n",
    "\n",
    "for i in range(0, num_plots, cols_per_fig):\n",
    "    fig, axes = plt.subplots(1, cols_per_fig, figsize=(20, 4))\n",
    "    for j, ax in enumerate(axes):\n",
    "        idx = i + j\n",
    "        if idx < num_plots:\n",
    "            col = num_cols[idx]\n",
    "            sns.histplot(df[col], kde=True, bins=50, color=\"steelblue\", ax=ax)\n",
    "            ax.set_title(f\"Feature: {col}\")\n",
    "            ax.set_xlabel(col)\n",
    "            ax.set_ylabel(\"Fréquence\")\n",
    "        else:\n",
    "            ax.axis(\"off\")  # cache les axes vides\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes avec moins de 10% de Positif : 384 (84.77%)\n",
      "Colonnes avec moins de 20% de Positif : 426 (94.04%)\n",
      "Colonnes avec moins de 50% de Postif : 451 (99.56%)\n",
      "Colonnes avec plus de 50% de Postif : 2 (0.44%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/training_set_metadata.csv\", sep=';')\n",
    "\n",
    "binary_cols = [col for col in df.columns if df[col].dropna().nunique() == 2]\n",
    "\n",
    "percent_ones = df[binary_cols].mean()\n",
    "\n",
    "count_less_10 = (percent_ones < 0.10).sum()\n",
    "count_less_20 = (percent_ones < 0.20).sum()\n",
    "count_less_50 = (percent_ones < 0.50).sum()\n",
    "count_more_50 = (percent_ones >= 0.50).sum()\n",
    "\n",
    "total_binary_cols = len(binary_cols)\n",
    "\n",
    "print(f\"Colonnes avec moins de 20% de Positif : {count_less_20} ({count_less_20 * 100 / total_binary_cols:.2f}%)\")\n",
    "print(f\"Colonnes avec moins de 50% de Postif : {count_less_50} ({count_less_50 * 100 / total_binary_cols:.2f}%)\")\n",
    "print(f\"Colonnes avec plus de 50% de Postif : {count_more_50} ({count_more_50 * 100 / total_binary_cols:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes avec moins de 20% de Positif : 314 (90.23%)\n",
      "Colonnes avec moins de 50% de Postif : 343 (98.56%)\n",
      "Colonnes avec plus de 50% de Postif : 5 (1.44%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"final_pred.xlsx\")\n",
    "\n",
    "binary_cols = [col for col in df.columns if df[col].dropna().nunique() == 2]\n",
    "\n",
    "percent_ones = df[binary_cols].mean()\n",
    "\n",
    "count_less_10 = (percent_ones < 0.10).sum()\n",
    "count_less_20 = (percent_ones < 0.20).sum()\n",
    "count_less_50 = (percent_ones < 0.50).sum()\n",
    "count_more_50 = (percent_ones >= 0.50).sum()\n",
    "\n",
    "total_binary_cols = len(binary_cols)\n",
    "\n",
    "print(f\"Colonnes avec moins de 20% de Positif : {count_less_20} ({count_less_20 * 100 / total_binary_cols:.2f}%)\")\n",
    "print(f\"Colonnes avec moins de 50% de Postif : {count_less_50} ({count_less_50 * 100 / total_binary_cols:.2f}%)\")\n",
    "print(f\"Colonnes avec plus de 50% de Postif : {count_more_50} ({count_more_50 * 100 / total_binary_cols:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
